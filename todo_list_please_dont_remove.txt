
To-do list for SPECFEM3D_GLOBE, by Dimitri Komatitsch and David Michea
----------------------------------------------------------------------

# enlever blocs AB, AC et BC partout, puisque tous les blocs sont maintenant de même type (*mais* il faudra quand même laisser les six noms : CHUNK_AB, CHUNK_AC, CHUNK_AB_ANTIPODE etc et garder les tests de type if(ichunk == CHUNK_AB_ANTIPODE) then partout où il y en a, car on continue à traiter 6 blocs, même s'ils ont maintenant la même topologie. Il faut donc conserver tout ce qui concerne ces six noms de blocs -> {ok pour le mailleur, reste la différentiation dans le solveur, mais sans conséquences. A mettre à plat lors de la fusion. DM.}

# enlever les "modules" et "use" partout où il y en a (pour anisotropie etc) car ce n'est pas "thread safe" et donc ça se plantera si on utilise MPC au lieu de MPI. Remplacer par la transmission explicite des paramètres aux subroutines. Peut-être dire dans le manuel utilisateur qu'il ne faut pas utiliser de modules pour cette raison ? -> {OK. DM.}

# ajouter le nombre analytique de points de GLL (appelé NGLOB) dans le mailleur (dans le nouveau read_compute_parameters.f90) -> {OK. DM.}

- merge the mesher in the solver, suppress storage of large mesh files on local disks, but make sure we keep the creation of AVS/OpenDX mesh files (which should be the case since we use specific routines called write_AVS_DX_*.f90)

# modifier_creation_central_cube_now_32_au_lieu_de_16 -> {ok, mais a revoir : 2**4=16. DM.}

- make_sure_aniso_elements_created_first

- utiliser auto_NER de Brian Savage pour determiner les constantes d'attenuation pour les chunks complets et non pas seulement si différent de 90 degrés. Pour cela, supprimer mes vieilles variables MIN_ATTENUATION et MAX_ATTENUATION_PERIOD

# modifier_create_header_file pour s'assurer qu'il fonctionne avec le nouveau mailleur -> {OK reste à enlever les appels à exit_mpi. DM.}

- implement classical Cuthill-McKee sorting

- ajouter une routine qui mette tous les gros tableaux à zéro avant le début
de la boucle en temps dans le solver afin d'éviter les effets du "First Touch"
de Linux

- use indirect addressing (inverse permutation array) and a flag to store anisotropy in the mantle to avoid having to start with that region, which breaks the Cuthill-McKee sorting; once this is done, suppress the statement in which we make this region become region number 1 in the mesh creation routine

# dans constants.h il y a un flag IFLAG_220_MOHO qu'il faudra séparer en deux en
créant IFLAG_220_80 et IFLAG_80_MOHO. Facile dans constants.h,
mais partout où apparaît ensuite IFLAG_220_MOHO par exemple
dans des if dans tous les *.f90 il faudra changer tous les
if(machintruc == IFLAG_220_MOHO) then
en
if(machintruc == IFLAG_220_80 .or. machintruc == IFLAG_80_MOHO) then
Faudra qu'on fasse ça à l'occasion ok ? par ex la semaine prochaine,
c'est rapide à faire.
(c'est Brian Savage, qui travaille sur l'atténuation dans le code,
qui a besoin de distinguer ces deux sub-flags) -> {OK, a revoir qd meme. DM.}

- rewrite communications entirely (use clean non-blocking MPI communications, use persistent communications, hide communications in calculations); see how to do this in the central cube, which is more difficult because it is in contact with many slices (see routine create_central_cube_buffers.f90). Split central cube in 2 with MPI to have better load balancing there and remove bottleneck detected by ORNL in the MPI implementation of the central cube (many slices send to one processor, which creates a big bottleneck). Do this using the code developed by Nicolas Le Goff for SPECFEM3D_BASIN with Céline. Ajouter un flag (appelé PERFECT_LOAD_CENTRAL_CUBE) pour rendre le load balancing parfait dans le central cube en calculant pour rien dans les 4 cubes marqués "FICTITIOUS" mais en n'utilisant pas les résultats dans la sommation.

? use BLAS3 calls in compute_forces*.f90 instead of loops as in the current 
implementation. We should gain a huge factor if we can do this, but we will
need to change the structure of the arrays. Mais Dimitri a deja teste BLAS3
et on ne gagne rien, en fait on perd meme des perfs, donc voir avec Dimitri
s'il faut laisser tomber ce point et laisser les americains de Oak Ridge
le faire s'ils veulent, mais sans nous -> {OK, les américains s'en occupent. DM.}

- supprimer write_AVS_mesh_quality, check_buffers*.f90, check_mesh_quality*.f90 etc du serveur SVN (svn remove) une fois que le mesher et le solver auront été fusionnés. Les enlever du Makefile.in aussi. Do not store x y z in the files that contain the MPI buffers anymore, since they are used only by check_buffers*. (in the solver they are read back as xdummy, ydummy, zdummy); eventuellement supprimer aussi le fichier ecrit par le mailleur sur le disque et relu plus tard par ce meme mailleur, s'il n'est pas utilise par le solver (c'est a dire s'il est utilise seulement par les codes serial
check*.f90, qui sont amenes a disparaitre)

- afficher le modele PREM,les courbes de nombre de points par longueur d'onde et de stabilité et les worst elements (stability et nb pts per lambda) en OpenDX en serial sur le maître dans le nouveau mesher, en copiant/collant ce qu'il faut depuis display_prem_sampling_doubling.f90 sur le master seulement (rank == 0)

- use MPC in the code instead of MPI, if we can get MPC from CEA under GPL or CeCILL licence. Make sure the code is thread-safe (for instance suppress all the modules and the "use" statements, which are not thread-safe); on saura le 15 mars ou en novembre (les deux prochaines reunions NUMASIS) si on peut recuperer le code MPC en licence open source de chez Dominique Rodriguez au CEA

- modifier write_seismograms.f90 pour que tout soit écrit directement sur le home par le maître et non pas sur les disques locaux par chaque noeud. Pour cela faire des paires SEND/RECV en allouant dynamiquement un tableau de taille différente à chaque fois (de taille nrec_local, sauf si nrec_local = 0, dans ce cas on n'envoie/reçoit rien pour cette tranche qui ne contient pas de récepteur). A la fin on doit avoir exactement 3*765 = 2295 fichiers (765 stations dans DATA/STATIONS, chacune stockant les trois composantes du vecteur déplacement)

- dans specfem3D.f90 ci-dessous à un moment on appelle deux fois la routine d'assemblage MPI, une fois pour assembler le crust + mantle et une fois pour assembler le inner core, juste après, ce qui double le nombre de messages et la latence pour rien. Donc quand on réécrira le MPI du solver bien entendu il faudra penser à fusionner les deux en un seul envoi de messages contenant crust + mantle + inner core. Voici le code obsolète ci-dessous.
! crust and mantle
  call assemble_MPI_vector(myrank,accel_crust_mantle,nglob_crust_mantle, & ...
! inner core
  call assemble_MPI_vector(myrank,accel_inner_core,NGLOB_INNER_CORE, & ...

- compiler l'ensemble du code sous gfortran, pgf90 (par ex sur ipigps014), Intel ifort (par ex sur pangu), IBM xlf à l'Idris afin de vérifier avant le commit que le code ne donne aucun warning avec ces quatre compilos classiques, ce qui permet de vérifier la portabilité du code final

- utiliser netCDF pour stocker du binaire partout où c'est nécessaire. voir si ça vaut le coup de stocker les gros fichiers de topographie dans DATA/topo_bathy dans ce format ou non, afin de réduire leur taille

- run Sdiff calculations for Lev Vinnik (Moscow, Russia)

- run 3D PKP calculations for Sebastien Chevrot (Toulouse, France); Dimitri va faire un code 2D serial pour etudier le modele et definir le maillage avant de lancer de gros runs a Barcelone

- carefully study and improve the stability condition and increase/adapt time step DT accordingly (do this with Dimitri, by trial and error)

- develop two-level Cuthill-McKee sorting for the two levels of cache with Jose Maria Cela (Barcelona); write a paper in "Parallel Computing" about this

- maybe use VMX/Altivec instructions in compute_forces*.f90 specifically for
  MareNostrum with Jose Maria Cela (Barcelona) (but only if using BLAS3 calls
  in compute_forces*.f90 turned out to be impossible; otherwise it is better
  to call BLAS3, which will already be optimized by IBM for VMX/Altivec)

- distinguer_DT_suivant_si_honor_PREM_or_not

- envoyer nvx code après cela barcelona à levesque ORNL pour optimisation BLAS3 s'ils y arrivent

- Virer variables intermédiaires dans compute_forces dans codes 2D et 3D (utiliser timing 23 sec 26 sec qui est dans le directory BLAS3_SPECFEM)

- modifier RMOHO_FICTITIOUS_IN_MESHER = R80 + (R_EARTH - R80) * dble(NER_80_MOHO) / dble(NER_80_MOHO + NER_CRUST)
!!!!!!!!!!!!!!! DK DK ceci a revoir, voir ce qu'on fait dans ce cas-la
    ONE_CRUST = .false.
dans display_prem_sampling_doubling.f90 et dans le nouveau code de maillage de David

- verifier / ameliorer HONOR_1D_SPHERICAL_MOHO (pas vraiment traite actuellement)

- implement better (partly inflated) central cube

- modifier le calcul des bornes de l'attenuation: car
    MIN_ATTENUATION_PERIOD   = 20
    MAX_ATTENUATION_PERIOD   = 1000
est arbitraire dans read_compute_parameters.f90 actuellement, utiliser la routine plus generale de Brian Savage.

- add UPPA logo and name to frontpage of the manual, add David to the list of contributors, mention UPPA in the main text of the manual; also add UPPA to copyright info and to subroutine headers

- regarder consequences de multiple de 32 au lieu de 16 dans table 3.1 du manuel, et refaire cette table en fonction de cette nouvelle contrainte. Mettre à jour la Table 3.1 avec multiples de 32 au lieu de 16 et aussi en disant qu'on peut mettre 2 ou 3 ou 6 tâches par proc et donc que le nb de procs donnés peut être divisé par 2, 3 ou 6 et donc limitation moins grande qu'on ne pourrait le croire (dire que c'est toujours divisible car NPROC = 6 N^2 donc divisible par 6); dire cela dans le texte principal du manuel et dans la légende de la table, par exemple "We should also keep in mind (and mention in the manual) that if 6 N^2 is a limitation on some machines, one can always run 2, 3 or 6 tasks per processor and therefore run on N^2, 2 N^2 or 3 N^2 processors. I will add a comment about this in the manual (probably in the caption of Table 3.1)."

- ajouter les courbes de dispersion et stabilité au manuel pour tous les NEX classiques (160, 256, 380 etc) dans un Appendix spécial là-dessus; dire dans cet appendix que dans le outer core ce n'est bien sûr par Vs qui est représenté mais Vp / 1.25 (on a pris 25 % de marge car on travaille en velocity potential dans le outer core, ce qui demande un échantillonnage un peu meilleur)

- dire dans manuel que number of grid points per S wavelength (purposely) pas bon dans le inner core, donc ne pas utiliser ce code pour étudier PKJKP ou bien dans ce cas prendre un maillage prévu pour une période beaucoup plus courte (typiquement deux fois) que la période mentionnée dans les tables, qui concerne seulement les ondes P et non pas les S dans le inner core

- supprimer write_AVS_mesh_quality, check_buffers*.f90, check_mesh_quality*.f90 etc du manuel une fois que le mesher et le solver auront été fusionnés (enlever les deux sections expliquant l'utilisation de ces routines)

- tester l'utilisation de netCDF et nous donner les routines pour lire/ecrire du binaire à ce format depuis SPECFEM3D_BASIN. David utilisera ensuite cela depuis SPECFEM3D_GLOBE.

- etudier comment lire un maillage CUBIT stocké au format natif de CUBIT
(Exodus, basé sur netCDF) depuis SPECFEM3D_BASIN. On pourrait utiliser la
commande "ncdump" dans CUBIT si nécessaire d'après Emanuele Casarotti. Une
autre option serait d'utiliser le format de stockage ABAQUS dans CUBIT.

- dire a Brian Savage (Washington D.C.) de faire un auto_NER a partir de PREM (choix automatique des parametres) a partir de display_prem_sampling_doubling.f90 dans DATA/util/mesh_doubling_superbrick
Subject: Re: attenuation (other idea)
From: Dimitri Komatitsch <dimitri.komatitsch@univ-pau.fr>
Date: Sun, 21 Jan 2007 21:59:54 +0100
To: Brian Savage <savage@uri.edu>
CC: Jeroen Tromp <jtromp@gps.caltech.edu>, Qinya Liu <lqy@gps.caltech.edu>, Vala
Hjorleifsdottir <vala@gps.caltech.edu>, Ying Zhou <yingz@gps.caltech.edu>,
David Michéa <davidmichea@gmail.com>
Dear all,
Following Brian's tests described in his email below,
I think we should change the way we set the width of the
absorption band in read_parameters.f90. In the current implementation
we impose an arbitrary range, e.g. for NEX=160 we set:
    MIN_ATTENUATION_PERIOD   = 20
    MAX_ATTENUATION_PERIOD   = 1000
but of course we should replace this with some kind of automatic
range selection process based on the value of N_SLS (3, 4 or 5)
and on mesh resolution I guess.
Brian, if you have a way of adding this to the general 3D attenuation
routine you wrote, please do not hesitate to implement it
and get rid of the current arbitrary ranges... I think having
a flexible routine to do this would be very useful.
Dimitri.
Brian Savage wrote:
> Dear All,
> I will run some tests to determine the maximum width of the absorption band
> with 3,4 and 5 linear solids.  As I recall, the optimum for 3 was 1.75
> decades in the frequency domain.
> By the way, I think I have tracked down the differences between 3.5 and 3.6.
> I believe it has to do with how the sharp attenuation boundaries are
> represented.  More testing still to be done.
> Cheers,
> Brian
> On Jan 1, 2007, at 8:02 PM, Jeroen Tromp wrote:
>> Hi Dimitri:
>> I think this at least worth investigating, i.e., how much more memory does
>> it take, how much flatter is the Q, how much broader is the absorption
>> band, etc.
>> But we still need to fix the existing bug first....
>> Jeroen
>>> Dear all,
>>> By the way, talking about attenuation, now that memory is becoming less
>>> of an issue it could be clever to switch from 3 memory variables to 4
>>> (N_SLS = 4) in the global code at some point in the (near?) future. (in
>>> particular to make attenuation more precise for the dispersion of
>>> surface waves).
>>> Do you think this could be a good idea?
>>> Dimitri.
>>>

- Brian,
ok, perfect, thank you. I will modify the code accordingly,
to call your routine to define the attenuation parameters
automatically even in the case of the full Earth.
Cheers,
Dimitri.
Brian Savage wrote:
> Dimitri,
> short answer: yes
> long answer:
> The lines you are describing MIN_ATT and MAX_ATT are defined for specific
> number of elements NEX_MAX and assuming a 90 degree chunk.  You may suppress
> them if you wish, but you would need to remove the if(ANGULAR_WIDTH_ ...)
> line to make certain these numbers get set.
> Look at line numbers 333-369 or so in read_parameter_file.f90.  The equation
> for min and max attenuation period is in there along with the DT and radial
> distribution of elements.
> I would also suggest testing the code a bit more than I have to be certain
> that the values it returns are similar to the ones you wrote in a few years
> ago.  I think running xcreate_header_file is sufficient to do this.  The
> equations are a bit of a mess, especially the one determining the time
> stepping.  There are some hard coded numbers in there too, which could
> probably be cleaned up a bit.
> I hope that made sense.
> Cheers,
> Brian
> On Jan 26, 2007, at 11:20 AM, Dimitri Komatitsch wrote:
>> Dear Brian,
>> One quick question: do you still use
>> MIN_ATTENUATION_PERIOD   = 20
>> MAX_ATTENUATION_PERIOD   = 1000
>> that are set in read_parameters.f90, or can I suppress them?
>> If you still use them, could you write a routine to set them
>> to the right frequency band automatically? (the current implementation,
>> i.e. setting them to arbitrary values without adding a comment
>> or using a formula is not very easy to maintain: I remember writing these
>> lines a few years ago but I don't remember how I got the numerical
>> values!
>> Thank you,
>> Dimitri.
>>

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Done:
-----


