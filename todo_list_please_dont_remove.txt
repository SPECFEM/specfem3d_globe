
To-do list for SPECFEM3D_GLOBE, by Dimitri Komatitsch and David Michea
----------------------------------------------------------------------

Dimitri:
-------

- developper un code acoustique 3D pour le crust_mantle et ajouter une condition de Dirichlet pour la surface libre en acoustique pour le code PKP

- mettre modele d'aniso de Seb dans le inner core

- run Sdiff calculations for Lev Vinnik (Moscow, Russia)

- run 3D PKP calculations for Sebastien Chevrot (Toulouse, France)

David:
-----

- le flag pour le multilevel Cuthill-McKee qui est dans constants.h.in
n'est jamais utilisé (un grep ne le montre dans aucun fichier Fortran90),
il semble donc que soit on ne fait jamais de multilevel
cuthill soit on le fait toujours (et alors on ne fait jamais le cuthill
simple niveau cuthill). Il faudra corriger cela pour faire l'un ou l'autre
suivant la valeur de ce flag
dkomati1-ipigps014: grep -i MULTI_LEVEL_CUTHILL *.h* *90
constants.h.in:  logical, parameter :: MULTI_LEVEL_CUTHILL = .true.

- Subject: permutation et aniso
Date: Fri, 03 Aug 2007 13:03:51 +0200
From: Dimitri Komatitsch <dimitri.komatitsch@univ-pau.fr>
Organization: University of Pau, France
To: David Michéa <davidmichea@gmail.com>
Salut David,
j'ai réfléchi au coup de la permutation qui plantait et au fait que ça ne plante plus quand tu enlèves trois tableaux aniso (qui deviennent non permutés), je pense que ça ne peut pas être ça et que c'est un hasard : ça doit donner un résultat faux mais stable par hasard.  en effet, il n'est pas logique que ces tableaux ne soient pas permutés, car ce sont des propriétés de l'élément au même titre que les autres (rho, vp, vs etc) donc ils doivent être permutés pareil, sinon ils ne correspondent plus au bon élément à mon avis donc le bug est ailleurs, probablement un effet de bord caché.
comme tu as déjà cherché partout, je pense que le mieux à la rentrée en septembre sera de remettre à plat le truc en appelant deux fois la routine de création du maillage
au lieu d'une, comme on y avait pensé initialement :
1/ tu appeles create_regions pour créer la liste des éléments et les xstore ystore zstore, mais rien d'autre
2/ tu tries ça avec get_global, et tu crées la permutation en appelant get_perm
3/ l'étape 2/ t'as donné le nouvel ordre dans lequel il faut créer les éléments, donc tu rappeles create_regions mais en bouclant sur perm(ispec) au lieu de ispec ; et cette fois-ci en créant et sauvant tous les tableaux (rhostore, vpstore vstrore etc) comme actuellement; du coup tu as la garantie que tous les tableaux sont créés et sauvés dans le bon ordre, et il n'y a plus besoin de routine de permutation des tableaux, on pourra carrément les supprimer
on pourra voir ça tranquille début sept (je rentre le 6 sept), rien ne presse, en attendant dans SVN le Cuthill McKee est désactivé (flag à false par défaut) donc ça ne gêne aucun autre développeur donc pas de souci

- si jamais les modifications ci-dessus pour faire marcher Cuthill-McKee ne fonctionnent pas, on peut décider de supprimer complètement Cuthill-McKee car on sait que cela ne nous fera pas gagner énormément; à voir à la rentrée, suivant si David arrive à faire marcher les modifications ci-dessus ou non 

- rendre plus claire la partie du code qui définit ONE_CRUST, HONOR_1D_SPHERICAL_MOHO and CASE_3D, suite aux remarques de Bernhard Schubert

- bug movies detecte par Vala: code crashes (probably in compute_forces) when setting MOVIE_VOLUME = .true.  > Stack trace terminated abnormally.  > forrtl: severe (174): SIGSEGV, segmentation fault occurred

- ecrire les sismogrammes (et les allouer en memoire) bout a bout les uns a la suite des autres par tranches de taille NTSTEP_BETWEEN_FRAMES (ou bien un parametre de ce genre, a ajouter dans le Par_file) pour economiser (beaucoup de) memoire pour les runs avec beaucoup de stations sur un tres grand nombre de pas de temps (idee decrite par Dominique Rodrigues fin juillet lors d'une reunion avec lui)

- Split central cube in 2 with MPI to have better load balancing there and remove bottleneck detected by ORNL in the MPI implementation of the central cube (many slices send to one processor, which creates a big bottleneck). To do this, see the old (unfinished) implementation done by Dimitri in version 3.6 on pangu in directories /home/komatits/bubu_test_central_cube_AB_antipode_54procs and /home/komatits/ok_chunk_AB_central_cube

- couper la superbrick en deux au milieu dans les deux directions par MPI pour repasser de blocs de taille 32 à des blocs de taille 16; sinon voir les conséquences des blocs de 32: en coupant au milieu des superbricks, clarifier ce qui se passe : on doit toujours prendre des multiples de 32, mais on peut les couper deux fois plus; voir si la Table 3.1 du manuel est toujours valable, ou bien sinon comment il faut la modifier

- compare multi-level Cuthill-McKee of David to Peano space-filling curves found by Dimitri in the literature

- cuthill mckee permutation casse les deux couches aniso, et donc il faut trouver un autre moyen de les traiter et de sauver les tableaux c11 c12 etc qui sont très couteux; ne plus mettre les deux couches aniso dans les premiers elements, mais plutôt n'importe où dans le maillage à cause de cuthill mckee. Make sure anisotropic elements created first, or use indirect addressing (inverse permutation array) and a flag to store anisotropy in the mantle to avoid having to start with that region, which breaks the Cuthill-McKee sorting; once this is done, suppress the statement in which we make this region become region number 1 in the mesh creation routine

- create the mesh from bottom layer to top layer (it is created the other way around right now): this will be better for Cuthill-McKee, for graphics etc; DK DK: already done by David in Barcelona 2007 I think

- convert the "allocatable" arrays (including
2D arrays for the MPI buffers etc, because for very large
runs they are not that small) in the solver
from dynamic to static (easy to do: write their size
in values_from_mesher.h in the mesher; but there are
so many of them and so many cases (with/without attenuation,
anisotropy, absorbing edges etc.) that we need to be careful:
DK DK already done by David in Barcelona 2007 I think;
DK DK if not already done, then there is no real need to do it
because MPI buffers have a very minor impact on performance

- get rid of the bottleneck in locate_sources and locate_receivers
when one uses a huge number of sources (e.g. Vala -> 100000 sources)
or a very large number of stations (e.g. Bernhard -> 20000 stations):
in such a case there is a bunch of MPI_GATHERS which waste a lot
of memory and can even use all the available memory and make the run crash.
Vala partially fixed this problem by using subsets of 1000 stations, but after
careful analysis we have found a way of doing better and getting rid of all
the GATHERs and *all* the extra memory (this way we won't have to limit the number of sources to a maximum of 100000, as currently done in the solver).

- compiler l'ensemble du code sous gfortran, pgf90 (par ex sur ipigps014), Intel ifort (par ex sur pangu), IBM xlf à l'Idris afin de vérifier avant la conference aux USA mi-octobre que le code final ne donne aucun warning avec ces quatre compilos classiques, ce qui permet de vérifier la portabilité

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Things to do later:
-------------------

- afficher le modele PREM,les courbes de nombre de points par longueur d'onde et de stabilité et les worst elements (stability et nb pts per lambda) en OpenDX en serial sur le maître dans le nouveau mesher, en copiant/collant ce qu'il faut depuis display_prem_sampling_doubling.f90 sur le master seulement (rank == 0)

- see if this would be useful (probably NOT, according to the ParaVer analysis done in Barcelona): rewrite communications entirely (use clean non-blocking MPI communications, use persistent communications, hide communications in calculations); see how to do this in the central cube, which is more difficult because it is in contact with many slices (see routine create_central_cube_buffers.f90). Do this using the code developed by Nicolas Le Goff for SPECFEM3D_BASIN with Céline.

- utiliser routines auto_ner de Brian Savage pour calculer MIN_ATTENUATION et MAX_ATTENUATION au lieu de mettre des constantes arbitraires comme actuellement: utiliser auto_NER de Brian Savage pour determiner les constantes d'attenuation pour les chunks complets et non pas seulement si différent de 90 degrés. Pour cela, supprimer mes vieilles variables MIN_ATTENUATION et MAX_ATTENUATION_PERIOD Re: mesh_radial 22/05/2007 00:21 Dimitri, Sure.  I will commit those changes when I get the R_CENTRAL_CUBE section working.  The calculation of MIN_ATTENUATION_PERIOD is CHUNK_WIDTH = <user defined> [ deg ] NEX_MAX           = <user defined> [ elements ] KM_PER_DEGREE = 111.11 [ km / deg ] POINTS_PER_WAVELENGTH = 4 GLL_POINT_SPACING = 4 MIN_VELOCITY = 2.25 [ km / sec ] S wave veloicty CHUNK_WIDTH_KM    = CHUNK_WIDTH * KM_PER_DEGREE POINT_SPACING_KM = CHUNK_WIDTH_KM / ( NEX_MAX * GLL_POINT_SPACING ) MIN_PERIOD               = (POINT_SPACING_KM * POINTS_PER_WAVELENGTH) / MIN_VELOCITY MAX_ATTENUATION_PERIOD depends on MIN_ and the number of N_SLS.  The dependence on N_SLS will also be included in the code Brian On May 21, 2007, at 5:11 PM, Dimitri Komatitsch wrote: > Brian, > This is perfect! Thank you very much.  > One important thing: could you call your routine to compute > MIN_ATTENUATION_PERIOD and MAX_ATTENUATION_PERIOD in all cases > (even for 90 degrees and even for classical NEX such as 160, 320 etc)?  > Because the current values are completely obsolete (I computed > them in 2000 by trial and error, I don't even remember how...  > calling your clean routine would be *much* better); > please don't hesitate to do it and commit the changes.  > Thanks!  > Cheers, > Dimitri.
Modifier le calcul des bornes de l'attenuation: car
    MIN_ATTENUATION_PERIOD   = 20
    MAX_ATTENUATION_PERIOD   = 1000
est arbitraire dans read_compute_parameters.f90 actuellement, utiliser la routine plus generale de Brian Savage. Dire a Brian Savage (Washington D.C.) de faire un auto_NER a partir de PREM (choix automatique des parametres) a partir de display_prem_sampling_doubling.f90 dans DATA/util/mesh_doubling_superbrick
Subject: Re: attenuation (other idea)
From: Dimitri Komatitsch <dimitri.komatitsch@univ-pau.fr>
Date: Sun, 21 Jan 2007 21:59:54 +0100
To: Brian Savage <savage@uri.edu>
CC: Jeroen Tromp <jtromp@gps.caltech.edu>, Qinya Liu <lqy@gps.caltech.edu>, Vala
Hjorleifsdottir <vala@gps.caltech.edu>, Ying Zhou <yingz@gps.caltech.edu>,
David Michéa <davidmichea@gmail.com>
Dear all,
Following Brian's tests described in his email below,
I think we should change the way we set the width of the
absorption band in read_parameters.f90. In the current implementation
we impose an arbitrary range, e.g. for NEX=160 we set:
    MIN_ATTENUATION_PERIOD   = 20
    MAX_ATTENUATION_PERIOD   = 1000
but of course we should replace this with some kind of automatic
range selection process based on the value of N_SLS (3, 4 or 5)
and on mesh resolution I guess.
Brian, if you have a way of adding this to the general 3D attenuation
routine you wrote, please do not hesitate to implement it
and get rid of the current arbitrary ranges... I think having
a flexible routine to do this would be very useful.
Dimitri.
Brian Savage wrote:
> Dear All,
> I will run some tests to determine the maximum width of the absorption band
> with 3,4 and 5 linear solids.  As I recall, the optimum for 3 was 1.75
> decades in the frequency domain.
> By the way, I think I have tracked down the differences between 3.5 and 3.6.
> I believe it has to do with how the sharp attenuation boundaries are
> represented.  More testing still to be done.
> Cheers,
> Brian
> On Jan 1, 2007, at 8:02 PM, Jeroen Tromp wrote:
>> Hi Dimitri:
>> I think this at least worth investigating, i.e., how much more memory does
>> it take, how much flatter is the Q, how much broader is the absorption
>> band, etc.
>> But we still need to fix the existing bug first....
>> Jeroen
>>> Dear all,
>>> By the way, talking about attenuation, now that memory is becoming less
>>> of an issue it could be clever to switch from 3 memory variables to 4
>>> (N_SLS = 4) in the global code at some point in the (near?) future. (in
>>> particular to make attenuation more precise for the dispersion of
>>> surface waves).
>>> Do you think this could be a good idea?
>>> Dimitri.
Brian, ok, perfect, thank you. I will modify the code accordingly,
to call your routine to define the attenuation parameters
automatically even in the case of the full Earth.
Cheers,
Dimitri.
Brian Savage wrote:
> Dimitri,
> short answer: yes
> long answer:
> The lines you are describing MIN_ATT and MAX_ATT are defined for specific
> number of elements NEX_MAX and assuming a 90 degree chunk.  You may suppress
> them if you wish, but you would need to remove the if(ANGULAR_WIDTH_ ...)
> line to make certain these numbers get set.
> Look at line numbers 333-369 or so in read_parameter_file.f90.  The equation
> for min and max attenuation period is in there along with the DT and radial
> distribution of elements.
> I would also suggest testing the code a bit more than I have to be certain
> that the values it returns are similar to the ones you wrote in a few years
> ago.  I think running xcreate_header_file is sufficient to do this.  The
> equations are a bit of a mess, especially the one determining the time
> stepping.  There are some hard coded numbers in there too, which could
> probably be cleaned up a bit.
> I hope that made sense.
> Cheers,
> Brian
> On Jan 26, 2007, at 11:20 AM, Dimitri Komatitsch wrote:
>> Dear Brian,
>> One quick question: do you still use
>> MIN_ATTENUATION_PERIOD   = 20
>> MAX_ATTENUATION_PERIOD   = 1000
>> that are set in read_parameters.f90, or can I suppress them?
>> If you still use them, could you write a routine to set them
>> to the right frequency band automatically? (the current implementation,
>> i.e. setting them to arbitrary values without adding a comment
>> or using a formula is not very easy to maintain: I remember writing these
>> lines a few years ago but I don't remember how I got the numerical
>> values!
>> Thank you,
>> Dimitri.
>>

- merge the mesher in the solver, suppress storage of large mesh files on local disks, but make sure we keep the creation of AVS/OpenDX mesh files (which should be the case since we use specific routines called write_AVS_DX_*.f90) 30/06/2007 18:19 Hi Jeroen, Leif and David, I thought about merging the mesher and the solver, there might be a hidden difficulty because of memory allocation: the solver allocates almost all the available memory on a given CPU in static arrays, and the solver allocates a similar amount dynamically with allocate() statements. Therefore combining them would roughly double the required amount of memory and will not fit because the Par_file is tuned in order for the solver to use almost all the available memory on each CPU (in the case of large runs).  On systems (such as Linux) with a "first touch" memory allocation policy things should be ok because (if I understand correctly, but I am not sure) the system will not physically allocate the static memory for the solver until it is actually called, i.e. once the mesher has finished, even if both are combined in the same program.  But on other systems I guess we could have problems and the run would not start and exit with an "out of memory" error?  I don't know how to solve this problem. (*if* it needs to be solved; if *all* modern systems use a first touch policy, then we are all set; please let me know; I don't know much about operating systems). Leif, would you have any idea about how to handle this?  If the problem turns out to be too complex, I suggest *not* doing it in v4.0 and waiting until we start working on v4.1 after the October 9-11 workshop. Dimitri.

- supprimer write_AVS_mesh_quality, check_buffers*.f90, check_mesh_quality*.f90 etc du serveur SVN (svn remove) une fois que le mesher et le solver auront été fusionnés. Les enlever du Makefile.in aussi. Do not store x y z in the files that contain the MPI buffers anymore, since they are used only by check_buffers*. (in the solver they are read back as xdummy, ydummy, zdummy); eventuellement supprimer aussi le fichier ecrit par le mailleur sur le disque et relu plus tard par ce meme mailleur, s'il n'est pas utilise par le solver (c'est a dire s'il est utilise seulement par les codes serial check*.f90, qui sont amenes a disparaitre)

- Re: revision of shortest period estimates?  08/05/2007 14:51 Hi Jeroen, Yes, we need to check that. The code in UTILS/doubling_brick automatically computes the minimum period for 4 points per wavelength for each classical NEX, therefore we can just cut and paste the result in the manual. If we want 5 points per wavelength we can multiply the periods by 5/4. However we need to doublecheck because I don't remember if my code uses 4 points or 4 intervals (i.e. 5 points), therefore the current values are maybe already fine (if I used 4 intervals). I will doublecheck.  In any case, the sampling curves I sent you for v3.6 and v4.0 are valid anyway (they are computed using non-dimensional values, and then multiplied by 4, and would therefore be identical if I decided to multiply by 5 instead).  Dimitri.  Jeroen Tromp wrote: > Hi Dimitri: > If we are shooting for a minimum of 5 grid points per shortest wavelength, > perhaps we should increase our estimate of an accuracy at periods of 18 s > and longer for an NEX=240 run by 20% to 22 s? And perhaps NEX=320 is > accurate down to periods of 15 s rather than 12 s?

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Things to add later to the manual:
----------------------------------

- stp si tu peux répondre à ça et on mettra ta réponse dans la section "modèles 3D" du manuel Subject: SPECFEM3D - Models Date: Mon, 2 Jul 2007 17:08:01 +0200 From: Bernhard Schuberth <mail@bernhard-schuberth.de> To: Dimitri Komatitsch <dimitri.komatitsch@univ-pau.fr> References: <200706041627.21859.mail@bernhard-schuberth.de> <200707020852.01968.mail@bernhard-schuberth.de> <4688C215.6070208@univ-pau.fr> Hi Dimitri, I was checking the implementation of different 3D geodynamic models in my own version of SPECFEM3D and came across some questions regarding the flags ONE_CRUST, HONOR_1D_SPHERICAL_MOHO and CASE_3D (I am not completely sure about the philosophy behind those flags)

- add UPPA logo and name to frontpage of the manual, add David to the list of contributors, mention UPPA in the main text of the manual; also add UPPA to copyright info and to subroutine headers

- ajouter les courbes de dispersion et stabilité au manuel pour tous les NEX classiques (160, 256, 380 etc) dans un Appendix spécial là-dessus; dire dans cet appendix que dans le outer core ce n'est bien sûr par Vs qui est représenté mais Vp / 1.25 (on a pris 25 % de marge car on travaille en velocity potential dans le outer core, ce qui demande un échantillonnage un peu meilleur)

- dire dans manuel que number of grid points per S wavelength (purposely) pas bon dans le inner core, donc ne pas utiliser ce code pour étudier PKJKP ou bien dans ce cas prendre un maillage prévu pour une période beaucoup plus courte (typiquement deux fois) que la période mentionnée dans les tables, qui concerne seulement les ondes P et non pas les S dans le inner core

- dire ça dans le manuel :  grep -i xstore *f90 | grep -i double, grep -i xstore *f90 | grep -i custom, doit tjrs être en double dans le mailleur, mais sauvé en CUSTOM et relu en CUSTOM dans le solver car pas besoin de cettte précision

- Dire dans manuel que par défaut le mailleur tourne lentement pour optimiser par cuthill mckee et tri de ibool, mais flag dans DATA/Par_file pour le desactiver (appeler ce flag SLOWER_MESHER_FASTER_SOLVER par exemple)

- explain this in the manual: We have implemented a simple way of handling the ONE_CRUST / two crusts issue : we let the user decide. In the new version, the models in Par_file (isotropic_prem, ak135 etc) always implement the right version with two crusts. But we have added new model names: isotropic_prem_one_crust, ak135_one_crust etc., in which case the code uses ONE_CRUST. Depending on the option chosen by the user, the code automatically selects the right time step (i.e., reduces the time step in the case of two crusts and a "small" NEX, to remain stable). This way, users can decide what they want to do.

- supprimer write_AVS_mesh_quality, check_buffers*.f90, check_mesh_quality*.f90 etc du manuel une fois que le mesher et le solver auront été fusionnés (enlever les deux sections expliquant l'utilisation de ces routines)

- add this to the manual: let us recall that in SPECFEM3D we do not use our GJI 2002 formulation, we use a displacement potential in the fluid (rather than a velocity potential as in our GJI). This leads to the fluid-solid matching condition with no iterations at the CMB and ICB introduced by Chaljub and Valette (2004).  (see also Tarje's comment about this after his eq (49) in the attached PDF). I will update the comments in the source code to mention that three years ago we switched to a displacement potential with no iterations.

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

DONE:
-----

- bug estimation mémoire NEX=1248 (marche) à NEX=1664 (plante) à résoudre: passer tout en double precision ou bien integer(kind=8): done by Dimitri

- fusionner en une seule étape le assemble_MPI de inner core et de crust_mantle merge assemble_MPI_vector for crust/mantle with assemble_MPI_vector for inner core in order to divide the number of MPI calls by 2 for the solid regions; car dans specfem3D.f90 ci-dessous à un moment on appelle deux fois la routine d'assemblage MPI, une fois pour assembler le crust + mantle et une fois pour assembler le inner core, juste après, ce qui double le nombre de messages et la latence pour rien. Donc quand on réécrira le MPI du solver bien entendu il faudra penser à fusionner les deux en un seul envoi de messages contenant crust + mantle + inner core: done by Dimitri

- enlever blocs AB, AC et BC partout, puisque tous les blocs sont maintenant de même type (*mais* il faudra quand même laisser les six noms : CHUNK_AB, CHUNK_AC, CHUNK_AB_ANTIPODE etc et garder les tests de type if(ichunk == CHUNK_AB_ANTIPODE) then partout où il y en a, car on continue à traiter 6 blocs, même s'ils ont maintenant la même topologie. Il faut donc conserver tout ce qui concerne ces six noms de blocs -> {ok pour le mailleur, reste la différentiation dans le solveur, mais sans conséquences. A mettre à plat lors de la fusion. DM.}

- enlever les "modules" et "use" partout où il y en a (pour anisotropie etc) car ce n'est pas "thread safe" et donc ça se plantera si on utilise MPC au lieu de MPI. Remplacer par la transmission explicite des paramètres aux subroutines. Peut-être dire dans le manuel utilisateur qu'il ne faut pas utiliser de modules pour cette raison ? -> {OK. DM.}

- modifier RMOHO_FICTITIOUS_IN_MESHER = R80 + (R_EARTH - R80) * dble(NER_80_MOHO) / dble(NER_80_MOHO + NER_CRUST) dans display_prem_sampling_doubling.f90 et dans le nouveau code de maillage de David: done by David

- ajouter le nombre analytique de points de GLL (appelé NGLOB) dans le mailleur (dans le nouveau read_compute_parameters.f90) -> {OK. DM.}

- modifier_creation_central_cube_now_32_au_lieu_de_16 -> {ok, mais a revoir : 2**4=16. DM.}

- modifier_create_header_file pour s'assurer qu'il fonctionne avec le nouveau mailleur -> {OK reste à enlever les appels à exit_mpi. DM.}

- dans constants.h il y a un flag IFLAG_220_MOHO qu'il faudra séparer en deux en
créant IFLAG_220_80 et IFLAG_80_MOHO. Facile dans constants.h,
mais partout où apparaît ensuite IFLAG_220_MOHO par exemple
dans des if dans tous les *.f90 il faudra changer tous les
if(machintruc == IFLAG_220_MOHO) then
en
if(machintruc == IFLAG_220_80 .or. machintruc == IFLAG_80_MOHO) then
Faudra qu'on fasse ça à l'occasion ok ? par ex la semaine prochaine,
c'est rapide à faire.
(c'est Brian Savage, qui travaille sur l'atténuation dans le code,
qui a besoin de distinguer ces deux sub-flags) -> {OK, a revoir qd meme. DM.}

- modifier write_seismograms.f90 pour que tout soit écrit directement sur le home par le maître et non pas sur les disques locaux par chaque noeud. Pour cela faire des paires SEND/RECV en allouant dynamiquement un tableau de taille différente à chaque fois (de taille nrec_local, sauf si nrec_local = 0, dans ce cas on n'envoie/reçoit rien pour cette tranche qui ne contient pas de récepteur). A la fin on doit avoir exactement 3*765 = 2295 fichiers (765 stations dans DATA/STATIONS, chacune stockant les trois composantes du vecteur déplacement)

- remplacé :
 double precision :: Qs
par
 integer :: interval_Q
pour des soucis de lisibilité.

- utiliser netCDF pour stocker du binaire partout où c'est nécessaire. voir si ça vaut le coup de stocker les gros fichiers de topographie dans DATA/topo_bathy dans ce format ou non, afin de réduire leur taille; tester l'utilisation de netCDF et nous donner les routines pour lire/ecrire du binaire à ce format depuis SPECFEM3D_BASIN. David utilisera ensuite cela depuis SPECFEM3D_GLOBE  --> remarque de Dimitri ajoutée: probablement pas vraiment utile finalement, donc je supprime cette ligne des choses à faire

- carefully study and improve the stability condition and increase/adapt time step DT accordingly: done by David and Dimitri, by trial and error

- distinguer_DT_suivant_si_honor_PREM_or_not: done by David and Dimitri

- verifier / ameliorer HONOR_1D_SPHERICAL_MOHO: done by David and Dimitri

- implement better (partly inflated) central cube: done by David and Dimitri

- regarder consequences de multiple de 32 au lieu de 16 dans table 3.1 du manuel, et refaire cette table en fonction de cette nouvelle contrainte. Mettre à jour la Table 3.1 avec multiples de 32 au lieu de 16 et aussi en disant qu'on peut mettre 2 ou 3 ou 6 tâches par proc et donc que le nb de procs donnés peut être divisé par 2, 3 ou 6 et donc limitation moins grande qu'on ne pourrait le croire (dire que c'est toujours divisible car NPROC = 6 N^2 donc divisible par 6); dire cela dans le texte principal du manuel et dans la légende de la table, par exemple "We should also keep in mind (and mention in the manual) that if 6 N^2 is a limitation on some machines, one can always run 2, 3 or 6 tasks per processor and therefore run on N^2, 2 N^2 or 3 N^2 processors. I will add a comment about this in the manual (probably in the caption of Table 3.1)." --> finalement pas nécessaire, car on a trouvé un moyen de refaire des blocs de 16 en coupant la superbrick en deux au milieu dans les deux directions

- implement classical Cuthill-McKee sorting: done in Barcelona 2007

- ajouter une routine qui mette tous les gros tableaux à zéro avant le début de la boucle en temps dans le solver afin d'éviter les effets du "First Touch" de Linux : pas utile et dangereux donc finalement on ne le fait pas

- use MPC in the code instead of MPI : on a fait des tests mais finalement on ne gagne pas donc on n'utilisera pas MPC dans le code, on reste en MPI

- portage du code sur le processeur CELL, à faire avec Jesus Labarta de Barcelone: fait par Dimitri a Barcelone 2007 en reecrivant le code principal en C

- envoyer nvx code après cela barcelona à levesque ORNL pour optimisation BLAS3 s'ils y arrivent : fait par Dimitri suite a Barcelone 2007

- etudier comment lire un maillage CUBIT stocké au format natif de CUBIT (Exodus, basé sur netCDF) depuis SPECFEM3D_BASIN. On pourrait utiliser la commande "ncdump" dans CUBIT si nécessaire d'après Emanuele Casarotti. Une autre option serait d'utiliser le format de stockage ABAQUS dans CUBIT : passe dans la todo list du code de bassin, car ca ne concerne pas le code global

- develop two-level Cuthill-McKee sorting for the two levels of cache with Jose Maria Cela (Barcelona): done by David and Carlos Egido in Barcelona 2007

- Virer variables intermédiaires dans compute_forces dans codes 2D et 3D : fait par Dimitri a Barcelona 2007, dans le cadre de l'ecriture des 18 versions serial pour comparaison

- mettre l'attenuation dans le inner core pour run PKP Seb: not needed, attenuation was already implemented in the inner core in version 3.6

- maybe use VMX/Altivec instructions in compute_forces*.f90 specifically for MareNostrum with Jose Maria Cela (Barcelona) (but only if using BLAS3 calls in compute_forces*.f90 turned out to be impossible; otherwise it is better to call BLAS3, which will already be optimized by IBM for VMX/Altivec): done by Dimitri in Barcelona 2007

- Dimitri va faire un code 2D serial pour etudier le modele PKP et definir le maillage avant de lancer de gros runs a Barcelone: not needed, we decided to run directly in 3D rather than first testing in 2D

- use BLAS3 calls in compute_forces*.f90 instead of loops as in the current implementation. We should gain a huge factor if we can do this, but we will need to change the structure of the arrays: done by Dimitri in Barcelone 2007

- remove derived types (for attenuation, for anisotropy etc) and remove -align sequence from the Makefile, because of potential portability and alignment problems on future machines; replace them with modules: not done because the current code is OK, therefore not a good idea to change it

- faire un commit du script de stretching de la croûte 3D de David dans UTILS: not needed anymore, because implemented in Fortran in stretching_function.f90

