
To-do list for SPECFEM3D_GLOBE, by Dimitri Komatitsch:
------------------------------------------------------

Things that could be done in a future version:

- merge SPECFEM3D_GLOBE into SPECFEM3D, i.e. create a single package and see the mesh of the Earth as a general mesh that we would decompose in parallel using PT-Scotch. Problem: would this scale for huge global Earth simulations, e.g. at a seismic period of 1 second on 200,000 processor cores?

- use a more accurate time scheme: either symplectic schemes (but there could be a problem in the viscoelastic case, see below) or else Runge Kutta RK4, which is non staggered and would thus solve the approximate implementation of the displacement gradient that I used to implement attenuation back in 1999.

Subject: Re: SEM attenuation
Date: Fri, 23 Jul 2010 12:16:40 +0200
From: Tarje Nissen-Meyer <tarjen@ethz.ch>
Organization: ETH Zurich
To: Dimitri Komatitsch <dimitri.komatitsch@univ-pau.fr>
CC: <yingz@vt.edu>, Jeroen Tromp <jtromp@princeton.edu>, Min Chen	<mchen@gps.caltech.edu>, Vala Hjorleifsdottir <vala@ldeo.columbia.edu>,	"Brian Savage" <savage@uri.edu>, Shiann-Jong Lee <sjlee@earth.sinica.edu.tw>,	"Roland Martin" <roland.martin@univ-pau.fr>, Bernhard Schuberth	<mail@bernhard-schuberth.de>, Carl Tape <carltape@fas.harvard.edu>, "Anne Sieminski" <anne.sieminski@obs.ujf-grenoble.fr>, Paul Friberg	<p.friberg@isti.com>, Kasper van Wijk <kasper@cgiss.boisestate.edu>, "Dylan Mikesell" <dmikesell@cgiss.boisestate.edu>, Federica Magnoni	<federica.magnoni@ingv.it>, Ebru Bozdag <bozdag@princeton.edu>, Hejun Zhu	<hejunzhu@princeton.edu>, Pieyre Le Loher <pieyre.le_loher@inria.fr>,	Christina Morency <cmorency@Princeton.EDU>, Emanuele Casarotti	<emanuele.casarotti@gmail.com>, Piero Basini <basini@bo.ingv.it>, "Emmanuel Chaljub" <Emmanuel.Chaljub@obs.ujf-grenoble.fr>, Qinya Liu	<liuqy@physics.utoronto.ca>, Yang Luo <yangl@Princeton.EDU>, Daniel Peter	<dpeter@Princeton.EDU>

Hi all,

Just to add that symplectic schemes are based on conservative systems
(they approximate the Hamiltonian) and we don't know yet how they behave
for dissipative media. But still of course something to be added and
looked at. Should also give credit to Jean-Paul Ampuero who really first
suggested trying these schemes for elastodynamics with the SEM.

Best regards,
Tarje

On 23/07/10 02:21, Dimitri Komatitsch wrote:
>
> Hi Ying,
>
> What I know for sure if that back in 1999 when I developed the time
> scheme for attenuation I used a trick that made implementation much
> easier but that also makes the 4th order Runge Kutta (RK4) time scheme
> become second order only, i.e. RK2 instead of RK4. This means that for
> very long simulations (for instance multi-orbit surface waves)
> attenuation can become inaccurate because of a lack of accuracy in the
> time scheme (in which case reducing Delta_t purposely solves the
> problem, but of course makes the simulation more expensive).
>
> A way of solving this problem could be to switch to better time
> schemes such as the symplectic time scheme that Tarje introduced in a
> GJI paper a few years ago. Tarje and I should probably implement that
> in the code one day... (therefore I cc him; we talked about this at
> the EGU meeting in May)
>
> Thank you,
> Dimitri.

- use RK4 or symplectic time scheme (will be done by Tarje Nissen-Meyer and/or Jean-Paul Ampuero) (would be useful in the 2D version of the code as well):
Hi Jeroen, Perfect. I think talking to Jean-Paul Ampuero would be useful
as well because in Utrecht last year he had told us that
he had implemented some nice 4th-order symplectic schemes
in his version of SEM2D. Dimitri.
Jeroen Tromp wrote:
> Hi Dimitri:
> This is one of the first things Tarje and I plan to work on after he
> arrives.
> Jeroen
> Dimitri Komatitsch wrote:
>> Hi Jeroen,
>> I think the last important thing that is missing
>> in SPECFEM3D (and SPECFEM2D) is a fourth-order time scheme.
>> I think both Jean-Paul Ampuero and Tarje Nissen-Meyer
>> have worked on this, they will both be at Caltech
>> soon therefore maybe they could take care of adding it?
>> This would definitely increase the accuracy of very long
>> simulations (e.g. multi-orbit surface waves).
>> Dimitri.

- From Dimitri: there is something in SPECFEM3D_GLOBE that we noticed a few years ago
regarding attenuation but never fixed: on page 813 of our 1999 paper
I used a trick suggested by Robertsson et al. (1994)
to use a non-staggered Runge-Kutta (RK4) scheme for the attenuation
equations while using a staggered finite-difference (Newmark)
time scheme for all the other equations. 
(combining staggered and non-staggered formulations being difficult)
The trick works fine in most cases but there is a hidden problem for very long
simulations, and in particular for multi-orbit surface waves: 
because of that trick, the RK4 is not really a real Runge-Kutta scheme
(because grad(displacement) is used as a source but not known half way between
time steps, therefore I replaced it with an average between
t and t + Delta_t, which implies that the approximation is not fourth-order
accurate any more because of that smoothing/interpolation). Therefore
dispersion due to attenuation is not very accurately computed in the case of
very long runs; that matters mostly for surface waves.
We should fix that one day. The only thing to do would be to design a better time
integration scheme for the attenuation equation, without that trick; the rest
(Newmark etc) is fine and does not need to change.
Reply from Daniel Peter (Princeton University, USA) on February 23, 2010:
for higher orbit arrivals, we do might want to consider the symplectic time
schemes as well. Tarje made some tests and showed some "shocking" results
for longer distance paths. However, I am not sure if his scheme currently
considers purely elastic simulations, without incorporating attenuation.
It would be nice having both improved formulations included for those cases.
This seems to be a nice outlook for a future version update.

- use a potential of (rho * u) instead of u in the fluid, in case of
  fluid-fluid discontinuities

- make the code compatible with helioseismology / general Cowling formulation (will be done
  with Tarje Nissen-Meyer)

- could be done by Vala:
we could use heuristic rules to make source and receiver detection much
faster and make these routines use far less memory. For instance using the
latitude, longitude and depth of each source or receiver we can quickly
determine if there is no chance for this source or receiver
to be located in the current slice; then that processor would immediately
switch to the next source or receiver; this could speedup the whole process by
a factor of 10 to 100 I guess when many sources are used (e.g. Vala uses
50,000) and/or many stations (Bernhard uses 20,000). We would
get rid of the bottleneck in locate_sources and locate_receivers
when one uses a huge number of sources (e.g. Vala -> 100000 sources)
or a very large number of stations (e.g. Bernhard -> 20000 stations):
in such a case there is a bunch of MPI_GATHERS which waste a lot
of memory and can even use all the available memory and make the run crash.
Vala partially fixed this problem by using subsets of 1000 stations, but after
careful analysis we have found a way of doing better and getting rid of all
the GATHERs and *all* the extra memory (this way we won't have to limit the number of sources to a maximum of 100000, as currently done in the solver).


---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Things to do later:
-------------------

- afficher le modele PREM,les courbes de nombre de points par longueur d'onde et de stabilite et les worst elements (stability et nb pts per lambda) en OpenDX en serial sur le maitre dans le nouveau mesher, en copiant/collant ce qu'il faut depuis display_prem_sampling_doubling.f90 sur le master seulement (rank == 0)

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Things to add to the manual:
----------------------------

- ajouter les courbes de dispersion et stabilite au manuel pour tous les NEX classiques (160, 256, 380 etc) dans un Appendix special la-dessus; dire dans cet appendix que dans le outer core ce n'est bien sur pas Vs qui est represente mais Vp / 1.25 (on a pris 25 % de marge car on travaille en velocity potential dans le outer core, ce qui demande un echantillonnage un peu meilleur)

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Done:
-----

- Jeroen Ritsema (from Univ of Michigan, USA) has an interesting suggestion
(on February 22, 2010) for SPECFEM3D_GLOBE, an option that is currently missing
but that would be easy to add, and potentially very useful:
"If I may suggest one modification to specfem-3D: allow for 3D mantle
structure (i.e. s20rts) and a PREM crust.
This is helpful in isolating the mantle contribution to waveform anomalies.
Right now, crust2.0 structure is automatically included if the mantle model is
turned on." Solution implemented by Daniel Peter from Princeton Univ (USA) on February 23, 2010:
"I put an option such that if one appends "_1Dcrust" to the 3D model 
name, e.g. "s20rts_1Dcrust" instead of "s20rts" in the Par_file, it
will just take the crust from the corresponding 1D reference model
(which is PREM for his s20rts model). Daniel. "

- Purposely not done because we decided not to merge the mesher with the solver, after trying in version 4.1_beta and noticing that the merged code was difficult to write and to maintain: supprimer sections qui decrivent write_AVS_mesh_quality, check_buffers*.f90, check_mesh_quality*.f90 etc du manuel une fois que le mesher et le solver auront ete fusionnes

